{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting MNIST dataset\n",
    "Once I obtain the dataset, I use train_test_split to split the dataset to train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# This library contains images of 28x28 pixels.\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist['data'], mnist['target'], test_size=0.15, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Developing and Training: Decision Tree, Random Forest, and KNN\n",
    "Here I import the 3 models, fit them to the training set, then use a for statement to iterate through the 3 models and prints the accuracy score. <br>\n",
    "KNeighbors - K is the number of nearest neighbors. The number of neighbors is the deciding factor.<br>\n",
    "RandomForest - Large number of individual decision trees that operate as an ensemble<br>\n",
    "DecisionTree - A Decision Tree<br> \n",
    "<br>\n",
    "RandomForestClassifier operates the best out of the three classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=10)\n",
    "knn_clf.fit(X_train, y_train)\n",
    "\n",
    "rnd_clf = RandomForestClassifier()\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=10)\n",
    "tree_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier 0.9656190476190476\n",
      "RandomForestClassifier 0.9666666666666667\n",
      "DecisionTreeClassifier 0.8538095238095238\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score for above models\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (knn_clf, rnd_clf, tree_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing and Training: Hard and Soft Voting Classifiers\n",
    "Using the two forms of voting (soft and hard) and use the three previous models, then use the accuracy score stated above.<br>\n",
    "Hard voting - Picking the predictor with the highest number of votes<br>\n",
    "Soft voting - Combining the probabiltiies of each prediction in the models and picking the prediction with the highest total probability<br>\n",
    "<br>\n",
    "Both Voting Classifiers performed very closely both at 96%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier 0.9643809523809523\n"
     ]
    }
   ],
   "source": [
    "# Hard Voting\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "voting_hard_clf = VotingClassifier(\n",
    "    estimators=[('knn', knn_clf), ('rnd', rnd_clf), ('tree', tree_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "voting_hard_clf.fit(X_train, y_train)\n",
    "y_pred = voting_hard_clf.predict(X_test)\n",
    "print(voting_hard_clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier 0.9602857142857143\n"
     ]
    }
   ],
   "source": [
    "# Soft Voting\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "voting_soft_clf = VotingClassifier(\n",
    "    estimators=[('knn', knn_clf), ('rnd', rnd_clf), ('tree', tree_clf)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "voting_soft_clf.fit(X_train, y_train)\n",
    "y_pred = voting_soft_clf.predict(X_test)\n",
    "print(voting_soft_clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Developing and Training: Decision Tree with Bagging and Pasting, and Random Forest\n",
    "Using bagging and pasting(for pasting, set bootstrap=False) and use the DecisionTree Classifier in the model. Also use the RandomForest classifier model to fit the data. Then use the Accuracy score method to produce the accuracy score. <br>\n",
    "Bagging Classifier with Bagging - Through diversity of training sets passed to each predictor in the model. instances replaced after being drawn by a classifier <br>\n",
    "Bagging Classifier with Pasting - does not replace instances after being drawn<br>\n",
    "<br>\n",
    "Random Forest out performed the Bagging classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=1\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "pas_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=False, n_jobs=1\n",
    ")\n",
    "pas_clf.fit(X_train, y_train)\n",
    "\n",
    "rnd_clf = RandomForestClassifier()\n",
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier 0.8475238095238096\n",
      "BaggingClassifier 0.8445714285714285\n",
      "RandomForestClassifier 0.9665714285714285\n"
     ]
    }
   ],
   "source": [
    "# Accuracy score for above classifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (bag_clf, pas_clf, rnd_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Developing AdaBoost classifier and finding optimized using GridSearchCV\n",
    "The below parameters was gathered from https://stackoverflow.com/questions/32210569/using-gridsearchcv-with-adaboost-and-decisiontreeclassifier. I was not able to find much information at all regarding GridSearchCV (from slides, book, or searching GridSearchCV). The only information I could gather was from StackOverflow. base_estimator__max_depths and base_estimator__min_samples_leaf accesses the base estimator hyperparameters (DecisionTree Classifier). n_estimators and learning_rate tunes the AdaBoos Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier 0.872\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=None, random_state=42), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n",
    "\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "print(ada_clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'base_estimator__max_depth': [i for i in range (2, 11, 2)],\n",
    "    'base_estimator__min_samples_leaf': [5, 10],\n",
    "    'n_estimators': [10, 50, 250, 1000],\n",
    "    'learning_rate': [0.01, 0.1]}\n",
    "\n",
    "grid_search_ada = GridSearchCV(ada_clf, parameters, verbose=3, scoring='f1', n_jobs=-1)\n",
    "\n",
    "grid_search_ada.fit(X_train, y_train)\n",
    "y_pred = grid_search_ada.predict(X_test)\n",
    "print(grid_search_ada.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2741c62eeb6d3c2dbd0c707b9d4df7ca82c89a9cca53ee8435971b6b6816686"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
