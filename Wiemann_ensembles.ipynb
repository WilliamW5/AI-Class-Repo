{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting MNIST dataset\n",
    "Once I obtain the dataset, I use train_test_split to split the dataset to train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# This library contains images of 28x28 pixels.\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist['data'], mnist['target'], test_size=0.15, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Developing and Training: Decision Tree, Random Forest, and KNN\n",
    "Here I import the 3 models, fit them to the training set, then use a for statement to iterate through the 3 models and prints the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=10)\n",
    "knn_clf.fit(X_train, y_train)\n",
    "\n",
    "rnd_clf = RandomForestClassifier()\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=10)\n",
    "tree_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier 0.9656190476190476\n",
      "RandomForestClassifier 0.9667619047619047\n",
      "DecisionTreeClassifier 0.8553333333333333\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score for above models\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (knn_clf, rnd_clf, tree_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing and Training: Hard and Soft Voting Classifiers\n",
    "Using the two forms of voting (soft and hard) and use the three previous models, then use the accuracy score stated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_hard_clf = VotingClassifier(\n",
    "    estimators=[('knn', knn_clf), ('rnd', rnd_clf), ('tree', tree_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "voting_soft_clf = VotingClassifier(\n",
    "    estimators=[('knn', knn_clf), ('rnd', rnd_clf), ('tree', tree_clf)],\n",
    "    voting='soft'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'VotingClassifier' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\William\\Documents\\Python\\AI class\\Wiemann_ensembles.ipynb Cell 9'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/William/Documents/Python/AI%20class/Wiemann_ensembles.ipynb#ch0000008?line=4'>5</a>\u001b[0m clf\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/William/Documents/Python/AI%20class/Wiemann_ensembles.ipynb#ch0000008?line=5'>6</a>\u001b[0m y_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/William/Documents/Python/AI%20class/Wiemann_ensembles.ipynb#ch0000008?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(clf \u001b[39m+\u001b[39;49m clf\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m, accuracy_score(y_test, y_pred))\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'VotingClassifier' and 'str'"
     ]
    }
   ],
   "source": [
    "# Accuracy score for voting models\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (voting_hard_clf, voting_soft_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Developing and Training: Decision Tree with Bagging and Pasting, and Random Forest\n",
    "Using bagging and pasting(for pasting, set bootstrap=False) and use the DecisionTree Classifier in the model. Also use the RandomForest classifier model to fit the data. Then use the Accuracy score method to produce the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=1\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "pas_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=False, n_jobs=1\n",
    ")\n",
    "pas_clf.fit(X_train, y_train)\n",
    "\n",
    "rnd_clf = RandomForestClassifier()\n",
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier 0.8472380952380952\n",
      "BaggingClassifier 0.844\n",
      "RandomForestClassifier 0.9672380952380952\n"
     ]
    }
   ],
   "source": [
    "# Accuracy score for above classifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (bag_clf, pas_clf, rnd_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Developing AdaBoost classifier and finding optimized using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                                          learning_rate=0.5, n_estimators=200,\n",
       "                                          random_state=42),\n",
       "             param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AdaBoost classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "\n",
    "grid_search_ada = GridSearchCV(ada_clf, param_grid=parameters, scoring = 'roc_auc')\n",
    "grid_search_ada"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2741c62eeb6d3c2dbd0c707b9d4df7ca82c89a9cca53ee8435971b6b6816686"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
