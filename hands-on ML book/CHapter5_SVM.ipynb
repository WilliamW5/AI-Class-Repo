{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM Classification\n",
    "<br>\n",
    "<img src=\"../img/large_mrg_cls.png\" alt=\"Large Margin Classification\" style=\"width: 500px;\"/> <br>\n",
    "Both these images are can be seperated by a line (linearly seperable). The left plot shows the deciswion boundaries of three possible linear classifiers. The model whose decisioun boundary is seperated by the dashed line is so bad that ist does not even seperate the classes properly. The other two models work perfectly on the training set. but their decision boundairies come so close to the instances that these models will probably not perform as well on new instances. <br><br>\n",
    "Large margin classification - fitting the widest possible street (represented by the parallel dashed lines) between the classes. <br><br>\n",
    "Support Vectors - adding more training instances \"off the street\" will not affect the decision boundary at all: it is fully determined (or \"supported\") by the instances located on the edge of the street. These are called Support Bectors (they are circles in the image above).<br> <br>\n",
    "<img src=\"../img/sensitivity_to_feat_scale.png\" alt=\"Sensitivity to feature scaling\" style=\"width: 500px;\"/> <br> \n",
    "SVMs are sensitive to to the feature scales. the vertical scale is much larger than the horizontal scale, so the widest possible street is close to hoizontal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Margin Classification\n",
    "hard margin classification - If we strictly impose that all instances must be off the street and on the right side, this is called hard margin classification. <br>\n",
    "Hard margin has 2 main issues: first, it only wirks if the data is linearly seperable, second, it is sensitive to outliers. See image below to see issue with hard margin.<br>\n",
    "<img src=\"../img/hard_margin_sens.png\" alt=\"Hard Margin Sensitivity\" style=\"width: 500px;\"/> <br>\n",
    "To avoid these issues, use a more flexible model. The objective is to find a good balance between keeping the street as large as possible and limiting the margin violations. This is called soft margin classification.<br>\n",
    "<br>\n",
    "When creating a SVM model using ScikitLearn, you can specify a number of hyperparameters. C is one of those hyperparameters. If youur SVM model is overfitting, you can try regularizing it by reducing C. If w eset it to a low value, then we end up with the model on the left. With a high value, we get the model on the right. Margin violations are bad. It's usually better to have few of them. However, in this case the model on the left has a lot of margin violations but will probably generalize better.<br>\n",
    "<img src=\"../img/c_hyperparameter.png\" alt=\"Reducing C Hyperparamert\" style=\"width: 500px;\"/><br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n"
     ]
    }
   ],
   "source": [
    "# Loads Iris dataset, scales the features, adn then trains a linear SVM model (using LinearSVC class with C=1\n",
    "# and the hinge-loss function) to detect Iris Virginica flowers\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2,3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) #Iris Virginica\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\"))\n",
    "])\n",
    "# Unlike Logistic Regression classifiers, SVM classifiers dow not output probabilities for each class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear SVM classification\n",
    "Linear SVM are efficient and work well with many datasets. However, a lot of datasets are not linear at all. To solve this we can add another feature, the resulting 2D dataset will then be perfectly linear, see below.<br>\n",
    "<img src=\"../img/nonlinear_svm.png\" alt=\"Nonlinear SVM\" style=\"width: 500px;\"/><br>\n",
    "<img src=\"../img/linearsvm_using_poly_features.png\" alt=\"LinearSVM using Polynomial Features\" style=\"width: 500px;\"/><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('poly_Features', PolynomialFeatures(degree=3)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('svm_clf', LinearSVC(C=10, loss='hinge'))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To implement a nonlinear SVM, create a pipeline containing a PolynomialFeatures transofrmer., followed by a \n",
    "# StandardScaler and a LinearSVC. \n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.15)\n",
    "polynomial_svm_clf = Pipeline([\n",
    "    (\"poly_Features\", PolynomialFeatures(degree=3)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    "])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Kernels\n",
    "A low polynomial degree cannot deal with very complex datasets. A high polynomial degree creates a huge number of features, making the model too slow. <br>\n",
    "However, while using SVMs, you can apply a mathematical technique called the kernel trick.<br>\n",
    "Kernel Trick - Makess it possible to get the same results if you had added many polynomia features, even with high-degree polynomials, without actually having to add them. Can be implemented by the SVC class. <br>\n",
    "<br>\n",
    "The code below's graph: <br>\n",
    "<img src=\"../img/svm_clf_poly_kernel.png\" alt=\"Polynomial clf with polynomial kernel\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=5, coef0=1, kernel='poly'))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trains a SVM classifier using a third-degree polynomial kernel.\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "])\n",
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Features\n",
    "Another technique to tackle nonlinear problems is to add features computed using a similarity function.<br>\n",
    "Similarity Function - Measures how much each instance resembles a particular landmark.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Regression\n",
    "To use SVM for regression instead of classification, the trick is to reverse the objective: instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM regression tries to fit as many instances as possible on the street while limiting margin violations (i.e., instances off the street). <br>\n",
    "\n",
    "<img src=\"../img/svm_reg.png\" alt=\"SVM Regression\" style=\"width: 500px;\"> <br>\n",
    "Adding more instances within the margin does not effect the model's predictions; thus, the model is said to be &epsilon;-insensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(epsilon=1.5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tackle linear SVM Regression\n",
    "# LinearSVR class is the regression equivalent to SVC class.\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "svm_reg = LinearSVR(epsilon=1.5)\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100, degree=2, kernel='poly')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tackle non-linear SVM regression\n",
    "# SVR class is the regression equivalent to SVC class.\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "362f10d2c1a231c17ac246becce2545daa81640b66c74881ccf242998023883a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
