{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM Classification\n",
    "<br>\n",
    "<img src=\"../img/large_mrg_cls.png\" alt=\"Large Margin Classification\" style=\"width: 500px;\"/> <br>\n",
    "Both these images are can be seperated by a line (linearly seperable). The left plot shows the deciswion boundaries of three possible linear classifiers. The model whose decisioun boundary is seperated by the dashed line is so bad that ist does not even seperate the classes properly. The other two models work perfectly on the training set. but their decision boundairies come so close to the instances that these models will probably not perform as well on new instances. <br><br>\n",
    "Large margin classification - fitting the widest possible street (represented by the parallel dashed lines) between the classes. <br><br>\n",
    "Support Vectors - adding more training instances \"off the street\" will not affect the decision boundary at all: it is fully determined (or \"supported\") by the instances located on the edge of the street. These are called Support Bectors (they are circles in the image above).<br> <br>\n",
    "<img src=\"../img/sensitivity_to_feat_scale.png\" alt=\"Sensitivity to feature scaling\" style=\"width: 500px;\"/> <br> \n",
    "SVMs are sensitive to to the feature scales. the vertical scale is much larger than the horizontal scale, so the widest possible street is close to hoizontal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Margin Classification\n",
    "hard margin classification - If we strictly impose that all instances must be off the street and on the right side, this is called hard margin classification. <br>\n",
    "Hard margin has 2 main issues: first, it only wirks if the data is linearly seperable, second, it is sensitive to outliers. See image below to see issue with hard margin.<br>\n",
    "<img src=\"../img/hard_margin_sens.png\" alt=\"Hard Margin Sensitivity\" style=\"width: 500px;\"/> <br>\n",
    "To avoid these issues, use a more flexible model. The objective is to find a good balance between keeping the street as large as possible and limiting the margin violations. This is called soft margin classification.<br>\n",
    "<br>\n",
    "When creating a SVM model using ScikitLearn, you can specify a number of hyperparameters. C is one of those hyperparameters. If youur SVM model is overfitting, you can try regularizing it by reducing C. If w eset it to a low value, then we end up with the model on the left. With a high value, we get the model on the right. Margin violations are bad. It's usually better to have few of them. However, in this case the model on the left has a lot of margin violations but will probably generalize better.<br>\n",
    "<img src=\"../img/c_hyperparameter.png\" alt=\"Reducing C Hyperparamert\" style=\"width: 500px;\"/><br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n"
     ]
    }
   ],
   "source": [
    "# Loads Iris dataset, scales the features, adn then trains a linear SVM model (using LinearSVC class with C=1\n",
    "# and the hinge-loss function) to detect Iris Virginica flowers\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2,3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) #Iris Virginica\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\"))\n",
    "])\n",
    "# Unlike Logistic Regression classifiers, SVM classifiers dow not output probabilities for each class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear SVM classification\n",
    "Linear SVM are efficient and work well with many datasets. However, a lot of datasets are not linear at all. To solve this we can add another feature, the resulting 2D dataset will then be perfectly linear, see below.<br>\n",
    "<img src=\"../img/nonlinear_svm.png\" alt=\"Nonlinear SVM\" style=\"width: 500px;\"/><br>\n",
    "<img src=\"../img/linearsvm_using_poly_features.png\" alt=\"LinearSVM using Polynomial Features\" style=\"width: 500px;\"/><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('poly_Features', PolynomialFeatures(degree=3)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('svm_clf', LinearSVC(C=10, loss='hinge'))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To implement a nonlinear SVM, create a pipeline containing a PolynomialFeatures transofrmer., followed by a \n",
    "# StandardScaler and a LinearSVC. \n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.15)\n",
    "polynomial_svm_clf = Pipeline([\n",
    "    (\"poly_Features\", PolynomialFeatures(degree=3)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    "])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Kernels\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "362f10d2c1a231c17ac246becce2545daa81640b66c74881ccf242998023883a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
