{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding Off topic\n",
    "import numpy as np\n",
    "\n",
    "a = np.arange(6).reshape(2, 3)\n",
    "print(a)\n",
    "print(a[:, np.newaxis, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fetch the data\n",
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = 'https://raw.githubusercontent.com/ageron/handson-ml2/master/'\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\") # Creates a path in the directory, consiting of a datasets folder, containing a housing folder\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for fetching the data\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the data into Pandas\n",
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetches and loads the data\n",
    "fetch_housing_data()\n",
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# income categories 1.5-6.0, this represents 15,000-60,000\n",
    "# Creates an income category attribute with 5 categories (labeled from 1 to 5): \n",
    "# category 1 ranges from 0 to 1.5 (i.e less than 15,000)\n",
    "# category 2 ranges from 1.5 to 3, and so on.\n",
    "import numpy as np\n",
    "housing['income_cat'] = pd.cut(housing['median_income'], bins=[0, 1.5, 3.0, 4.5, 6, np.inf], labels=[1, 2, 3, 4, 5])\n",
    "housing['income_cat'].hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified sampling based on the income category\n",
    "# stratum - a level or class to which people are assigned according to their social status, education, or income.\n",
    "# stratified - a common sampling technique used by researchers when trying to draw conclusions from different sub-groups or strata.\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing['income_cat']):\n",
    "    strat_train_set = housing.loc[train_index] # Here we get the trained set from the housing dataset\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "    \n",
    "strat_test_set['income_cat'].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes income_Cat attribute so the data is back to its original state:\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop('income_cat', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the axis attribute?\n",
    "I saw this a bunch of times, here is a reminder\n",
    "<img src=\"img/axis_attr.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing.plot(kind='scatter', x='longitude', y='latitude) - without alpha, cannot see the density\n",
    "housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4,\n",
    "            s=housing[\"population\"]/100, label='population', figsize=(10,7),\n",
    "            c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr() -computes the standard correlation coefficient (also called Pearson's r), between every pair of attributes\n",
    "# how much each attribute correlates with the median house value:\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Correlation coeficient\n",
    "The correlation coefficient ranges from -1 to 1. \n",
    "\n",
    "When the value is close to 1, it means there is a positive correlation. For example, the above situation you can see the median house value tends to go up when the median income goes up. \n",
    "\n",
    "When the value is close to -1, it means there is a negative correlation. For example, you can see a small negative correlation in latitude (i.e., when the prices have a slight tendency to go down when you go North).\n",
    "\n",
    "When the value is close to 0, it means there is no linear correlation. \n",
    "\n",
    "Again, this is only measures linear correlation. Not nonlinear.\n",
    "\n",
    "Here is the correlation coeficient when plotted. \n",
    "<img src=\"img/correlation_plot.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to check for correlation between attributes is to use the pandas: scatter_matrix() func\n",
    "# This plots every numerical attribute against every other numerical attribute.\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12,8))\n",
    "# The diagnol graphs (top-right -> bottom-left):\n",
    "# These are varaibles plotted against themselves. So instead of showing the correlation, Pandas dislpays a histogram\n",
    "# of each attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the plot just for median income. Showing a very strong (positive) correlation.\n",
    "housing.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new useful attributes \n",
    "housing[\"rooms_per_household\"] = housing['total_rooms']/housing['households']\n",
    "housing['bedrooms_per_room'] = housing['total_bedrooms']/housing['total_rooms']\n",
    "housing['population_per_household'] = housing['population']/housing['households']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view correlation\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix['median_house_value'].sort_values(ascending=False)\n",
    "# we can see houses with a lower bedroom/room ratio tend to be more expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops the median house value for the training set and assigns to housing variable\n",
    "# Copys the median house value to housing_labels to easy access.\n",
    "housing = strat_train_set.drop('median_house_value', axis=1)\n",
    "housing_labels = strat_train_set['median_house_value'].copy()\n",
    "# Here, we are predicting the median house values. So we drop it from the housing set, and add it to the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Drops the missing value datasets\n",
    "3 Options:\n",
    "    1. get rid of corresponding districts\n",
    "    2. get rid of whole attribute\n",
    "    3. set the values to some  value(zero, the mean, the median)\n",
    "    \n",
    "housing.dropna(subset=['total_bedrooms'])   # option 1\n",
    "housing.drop('[total_bedrooms'], axis=1)    # option 2\n",
    "median = housing['total_bedrooms'].median() # option 3\n",
    "housing['total_bedrooms'].fillna(median, inplace=True) \n",
    "\n",
    "When using drop only, it drops the columns/rows you define\n",
    "When using dropna, it removes all entries with NaN values (or null in general)\n",
    "\"\"\"\n",
    "# Scikit-LEarn provides a class to take care of missing values: SimpleImputer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Since median can only be calculated with numerical data, we have to drop ocean_proximity and calculate the median\n",
    "housing_numerical = housing.drop('ocean_proximity', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer simply computers the median of each attribute and stored the results in its statistics_ variable.\n",
    "imputer.fit(housing_numerical)\n",
    "\n",
    "# Provides the same information\n",
    "imputer.statistics_\n",
    "housing_numerical.median().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use this \"trained\" inputer to transform the training set by replacing missing values with the learned medians\n",
    "X = imputer.transform(housing_numerical)\n",
    "\n",
    "# The result above is a plain NumPy array containing the transformed features.\n",
    "# We can put the NumPy array back into a pandas DataFrame by doing the following:\n",
    "\n",
    "housing_tr = pd.DataFrame(X, columns=housing_numerical.columns, index=housing_numerical.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Design\n",
    "\n",
    "### Consistency:\n",
    ">Estimators:\n",
    "    >>Any object that can estimate some parameters based on a dataset is called an estimator( e.g., an imputer is an estimator). The estimation is performed by the fit() method, and it takes only a dataset as a parameter (or two for supervised learning algorithms; the second dataset contains the labels). Any other parameter needed to guide the estimation process is considered a hyperparameter (such as an imputer's strategy), and it must be aset as an instance varaible (generally via  a constructor parameter). \n",
    "    \n",
    ">Transformers:\n",
    "    >>Some estimators (such as an imputer) can also transform a dataset; these are called transformers. The transformation is performed by the transform() method with the dataset to transform as the parameter. All transformers also have a convenience method c alled fit_transform() that is equivalent to calling fit() and then transform() (but sometimes fit_transform() is optimized and runs much faster\n",
    "    \n",
    ">Predictors:\n",
    "    >>Finally, some estimators, given a dataset, are capable of making predictions; they are called predictors. For example, the LinearRegression model in the previous chapter was a predictor. A predictor has a predict() method that takes a dataset of new instances and returns a dataset of corresponding predictions. It also has a score() method that measures the quality of the predictions, given a test set( and the corresponding labels, in the case of supervised learning algorithms).\n",
    "    \n",
    "### Inspection:\n",
    ">All the estimator's hyperparameters are accessible directly via public instance vairiables (e.g., imputer.strategy), and all estimator's learned paramters are accessible via public instance variables with an underscore suffic (e.g., imputer.statistics_).\n",
    "\n",
    "### Nonproliferation of classes\n",
    ">Datasets are represented as NumPy arrays or SciPy sparse matricies, instead of homemade classes. Hyperparameters are just regular python strings or numbers.\n",
    "\n",
    "### Composition\n",
    ">Existing building blocks are reused as much as possible. For example, it is easy to create a pipeline estimator from an aribtrary sequence of transformers followed by an estimator.\n",
    "\n",
    "### Sensible Defaults\n",
    ">Scikit-Learn provides reasonable default values for most parameters, making it easy to quickly create a basline working system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested Brackets\n",
    "When you see df[['col_name']] you're really seeing:\n",
    "\n",
    ">col_names = ['col_name'] <br> df[col_names]\n",
    "\n",
    "In consequence, the only thing that [[ does for you is that it makes the result a DataFrame, rather than a Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing[['ocean_proximity']]\n",
    "housing_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The problem with Scikit-Learn is, it heavily requires numerical data to work with.\n",
    "# So we must convert the string to numbers, OrdinalEncoder in SKlearn helps with this\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the categories of the above dataset, use categories_\n",
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is OneHotEncoder?\n",
    "An issue with ML algorithms, they will assume that two nearby values are more similar than two distant values. But this isn't the case for ocean_proximity. As you can see above, cat 0 and cat 4 are for more related than 0 and 1. This image helps explain OneHotEncoder\n",
    "<img src=\"img/one_hot_encoder_example.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Transformers\n",
    "Many times you will need to write your own for tasks such as custom cleanup operations or combining specific attributes. You need the transofrmer to work seamlessly with SciKit-Learn functionalities (like pipelines) and since Scikit-Learn relies on duck typing (not inheritence), all you need to do is create a class and implement three methods: fit() (returning self), transform(), and fit_transform().\n",
    "\n",
    "> What is duck typing? <br> The idea of duck typing, is that you don't need a type in order to invoke an existing method on an object - if a method is defined on it, you can invoke it. <br> The name comes from the phrase \"If it looks like a duck and quacks like a duck, it's a duck\".\n",
    "\n",
    "You can obtain fit_transform() by adding TransformerMixin as a base class. If you add BaseEstimator as a base class, you will get two extra methods (get_params() and set_params() that will be useful for automatic hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "housing_extra_attribs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "There are two common ways to get all attributes to have the same scale: min-max scaling and standardization\n",
    "\n",
    ">min-max scaling (normalization) <br> min-max scaling (also called nomarlization) is the simplest: values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtracting the min value and dividing by the max minus the min. Scikit-LEarn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if, for some reason, you don't want 0 to 1.\n",
    "\n",
    ">standardization <br> standardization first subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation so that the resulting distribution has unit variance. Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1). However, standardizing is much less affected by outliers. For example, lets say you input a median income of 100 (by mistake). Min-max scaling would crush all the other values from 0-15 down to 0.15. But standardization it would be much less affected. Scikit learn provides StandardScaler for standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Gets a list of numerical column names (i.e., we already defined all numerical columns above)\n",
    "num_attribs = list(housing_numerical)\n",
    "# Gets a list of category colunmn names\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "# Handles all columns, applying the appropriate transformations to each column\n",
    "# The constructor requires a list of tuples, where each tuple contains a name, a transformer, \n",
    "# and a list of names (or indices) of columns that the transformer should be applied to.\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_attribs),   #returns a dense matrix\n",
    "    ('cat', OneHotEncoder(), cat_attribs) #returns a sparse matrix\n",
    "])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that everything has been preprocessed and played with the syntax, we can train a model.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# A reminder, housing_prepared does not include median house price, that exists in housing_labels.\n",
    "# So we fitting a model the solutions to the answers (housing_prepared=solutions, housing_labels=answer)\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "\n",
    "print(\"predictions: \", lin_reg.predict(some_data_prepared))\n",
    "print(\"labels: \", list(some_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEasure the regression model's RMSE on the whole training set using scikit-learns mean_Squared_error() function\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse\n",
    "\n",
    "# _rmse = typical prediction error. As of running this, 68,628. This means the model can be off by that amount\n",
    "\n",
    "# This shows the model underfitting the training data. This can mean that the features do not provide enough information\n",
    "# to make good predictions, or that the model is not powerful enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will train a Devision tree regressor, which is a more powerful model, capable of finding complex nonlinear\n",
    "# relationships in the data. \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "# Again, fitting the model by giving the solution and the answers\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation\n",
    "\n",
    "We can seriously doubt that the model is 100% accurate. So we can use tran_test_split and break the data up into smaller training set and a validation set. Then we can train the model against the smaller training set and evaluate them against the validation se.\n",
    "\n",
    "## HOWEVER!\n",
    "\n",
    "Scikit-learn provides an alternative, K-fold cross-validation feature. This randomly splits the training set into 10 distinct subsets called folds. Then it trains and evaluated the decision tree model 10 times, picking a different fold for ecaluation every time and trining on the other 9 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will implement cross-validation tool\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring='neg_mean_squared_error', cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard Deviation:\", scores.std())\n",
    "    \n",
    "display_scores(tree_rmse_scores)\n",
    "# The Decision Tree is performing worse than the linear model!\n",
    "# with a mean of 71270, +-2946"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets compute the same type of score for the linear model\n",
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring='neg_mean_squared_error', cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model, RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting and calculating the root mean square error\n",
    "housing_predictions = forest_reg.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the score\n",
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring='neg_mean_squared_error', cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)\n",
    "forest_rmse # Displays the prediction error, in this case, about $18000 off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of messing with hyperparameters manually, scikit-learn GridSearchSV can search for you.\n",
    "# Just tell which hyperparameter you wish to experiment with.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}\n",
    "]\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_\n",
    "# Output: {'max_features': 8, 'n_estimators': 30}\n",
    "grid_search.best_estimator_\n",
    "# Output: RandomForestRegressor(max_features=8, n_estimators=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the best features for accurate predictions\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the importance scores next to their corresponding attribute names\n",
    "extra_attribs = ['rooms_per_hhold', 'pop_per_hhold', 'bedrooms_per_room']\n",
    "cat_encoder = full_pipeline.named_transformers_['cat']\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the final mode - test set\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop('median_house_value', axis=1)\n",
    "y_test = strat_test_set['median_house_value'].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "# mean squared error, accepts answer, than solutions\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can computer 95% confidence interval for the generalization error using the following\n",
    "from scipy import stats\n",
    "confidence = 0.95\n",
    "squared_errors = (final_predictions - y_test) **2\n",
    "np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
    "                        loc=squared_errors.mean(),\n",
    "                        scale=stats.sem(squared_errors)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
