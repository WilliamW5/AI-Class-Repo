{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network (ANNs)\n",
    "Simulated using ANNs to perform various logical computations using propositional logic.<br>\n",
    "<img src=\"../img/anns_logic_computations.png\" alt=\"ANNs performing simple logical computations\" style=\"width: 500px;\"/><br>\n",
    "## Perceptron\n",
    "One of the simplest ANN architectures. It is based on another artificial neuron called a threshold logic unit (TLU), sometimes called a linear threshold unit (LTU): The inputs and outputs are row numbers (instead of binary on/off values) and each input connection is associated with a weight. The TLU computes a weighted sum of its inputs (z= w_1x_1 + w_2x_2 + ... + w_nx_n = x^Tw) then applies a step function to that sum and outputs the result h_w(x) = step(z), where z = x_tw. <br>\n",
    "<img src=\"../img/threshold_unit.png\" alt=\"Threshold Logic Unit\" style=\"width: 500px;\"/><br>\n",
    "The most common step function used in Perceptrons is the Heaviside step function. <br>\n",
    "<img src=\"../img/common_step_functions.png\" alt=\"Common Step Functions used in Perceptrons\" style=\"width: 500px;\"/><br>\n",
    "A Perceptron is composed of a single layer of TLUs, with each TLU connected to all the inputs. <br>\n",
    "When all neurons in single layer are connected to every neuron in the previous layer (i.e, its input neurons), it is called a **fully connected layer** or a **dense layer**.<br>\n",
    "A Percepton with two inputs and three outputs is represented below. This Perceptron can classify instances simultaneously into three different binary classes, which make it a multi-output classifer.<br>\n",
    "<img src=\"../img/perceptron_diagram.png\" alt=\"Perceptron with two inputs and three outputs\" style=\"width: 500px;\"/><br>\n",
    "<img src=\"../img/computing_output.png\" alt=\"Computing outputs of a fulyl connected layer\" style=\"width: 500px;\"/><br>\n",
    "<br>\n",
    "\"Cells that fire together, wire together\" Siegrid Lowel\n",
    "Also known as Hebb's rule (or Hebbian Learning). That is, the connection weight between two neurons is increased whenever they have the same output. <br>\n",
    "The Perceptron is fed one training instance at a time, and for each instance it makes its predictions. For every output neuron that produced a wrong prediction, it reinforces the connection weights from the inputs that would have contributed to the correct predictions.<br>\n",
    "<img src=\"../img/perceptron_learning_rule.png\" alt=\"Perceptron learning rule (weight update)\" style=\"width: 500px;\"/><br>\n",
    "<br>\n",
    "The decision boundary of each output neuron is linear, so Perceptroms are incapable of learning complex patterns(just like Logistic Regression Classifers). However, if the training instances are linearly seperable, Rosenblatt demonstrated that this algorithm would converge to a solution. This is called the **Perceptron Convergence theorem**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\William\\AppData\\Local\\Temp\\ipykernel_11040\\905234873.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = (iris.target == 0).astype(np.int) # Iris Setosa?\n"
     ]
    }
   ],
   "source": [
    "# Scikit-Learn provides a Perceptron class that implements a single TLU network.\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int) # Iris Setosa?\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The limitations of Perceptrons can be eliminated by stacking multiple Perceptrons. The resulting ANN is call a multi-layer Perceptron (MLP). In particular, an MLP can solve the XOR problem, as you can verify by computing the output of the MLP represented on the right of the figure below: with inputs (0, 0) or (1, 1) the network outputs 0, and with the inputs (0, 1) ir (1, 0) it outputs 1. All connections have weight equal to 1, except the four connections where the weight is shown. This network indeed solves the XOR problem! <br>\n",
    "<img src=\"../img/mlp_xor_problem.png\" alt=\"XOR classification problem and an MLP that solves it\" style=\"width: 500px;\"/><br>\n",
    "<br>\n",
    "### Multi-layer Perceptron and Backpropagation\n",
    "An MLP is composed of one (passthrough) input layer, one or more layers of TLUs, called hidden layers, and one final layer of TLUs called the output layer. The layers close to the input layers are usually called the **lower layers** and the ones closer to the output are called the **upper layers**. Every layer except the output layer includes a bias neuron and is fully connected to the next layer.<br>\n",
    "<img src=\"../img/mlp.png\" alt=\"Multi-Layer Perceptron\" style=\"width: 500px;\"/><br>\n",
    "<br>\n",
    "When an ANN contains a deep stack of hidden layers, it is called a deep neural network (DNN). Researchers struggled with training MLPs. When David Rumelhart, Geoffrey Hinton, and Ronald WIlliams introduced backpropagation training algorithm which is, in short, Gradient Descent using an efficient technique for computing the gradients automatically: in just two passes through the network (one forward, one backward), the backpropagation algorithm is able to compute the gradient of the netwrok's error with regards to every single model parameter. In other words, it can find out how each connection weight and each bias term should be tweaked in order to reduce the error. <br>\n",
    "Summarizing the algorith again: for each training instance the backpropagation algorithm first makes a prediction (forward pass), measures the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass), and finally slightly tweaks the connection weights to reduce the error (Gradient Descent step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an Image classifier p.294\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2741c62eeb6d3c2dbd0c707b9d4df7ca82c89a9cca53ee8435971b6b6816686"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
